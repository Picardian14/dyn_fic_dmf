{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_DYN_FIC_DMF'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loadmat\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfastdyn_fic_dmf\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdmf\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pool\n",
      "File \u001b[0;32m/network/iss/home/ivan.mindlin/dyn_fic_dmf/python/fastdyn_fic_dmf/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdyn_fic_dmf_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/network/iss/home/ivan.mindlin/dyn_fic_dmf/python/fastdyn_fic_dmf/dyn_fic_dmf_api.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mDynamic Mean Field model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03mPedro Mediano, Jun 2020\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01m_DYN_FIC_DMF\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     11\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault_params\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_DYN_FIC_DMF'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import math\n",
    "import time\n",
    "from scipy.io import loadmat\n",
    "import fastdyn_fic_dmf as dmf\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def compute_fcd(data, wsize, overlap, isubdiag, params):\n",
    "    T, N = data.shape\n",
    "    win_start = np.arange(0, T - params[\"wsize\"] - 1, params[\"wsize\"] - overlap)\n",
    "    nwins = len(win_start)\n",
    "    fcd = np.zeros((len(isubdiag[0]), nwins))\n",
    "    for i in range(nwins):\n",
    "        tmp = data[win_start[i]:win_start[i] + params[\"wsize\"] + 1, :]\n",
    "        cormat = np.corrcoef(tmp.T)\n",
    "        fcd[:, i] = cormat[isubdiag[0], isubdiag[1]]\n",
    "    return fcd\n",
    "\n",
    "def grid_step(args):\n",
    "    \"\"\"\n",
    "    Processes a single (DECAY, LR) pair with a specific seed and returns the results.\n",
    "    \"\"\"\n",
    "    (idx_DECAY, DECAY, LR, seed, params, nb_steps, burnout, overlap, isubfcd, G_range, node_strength, post_burnout_steps) = args\n",
    "    print(f\"Starting grid step: DECAY={DECAY}, LR={LR}, Seed={seed}\")\n",
    "    OBJ_RATE = 3.44\n",
    "    params['lrj'] = LR\n",
    "    params['taoj'] = DECAY\n",
    "    params['obj_rate'] = OBJ_RATE\n",
    "    params['seed'] = seed\n",
    "\n",
    "    # Initialize the results dictionary\n",
    "    result = {\n",
    "        'idx_DECAY': idx_DECAY,\n",
    "        'idx_LR': None,  # Will set after enumerating LR_range\n",
    "        'all_random_fic_cor': None  # Will hold the correlation data\n",
    "    }\n",
    "\n",
    "    # Iterate over G_range\n",
    "    all_random_fic_cor = np.zeros((len(G_range), post_burnout_steps), dtype=np.float32)\n",
    "\n",
    "    for idx_G, G_val in enumerate(G_range):\n",
    "        params['G'] = G_val\n",
    "        # Reference FIC calculation\n",
    "        reference_fic = 0.75 * params['G'] * params['C'].sum(axis=0).squeeze() + 1\n",
    "        # Create random vector in the same range as the reference fic\n",
    "        params['J'] = np.random.rand(params['C'].shape[0]) * reference_fic.max()\n",
    "\n",
    "        # Run the simulation\n",
    "        rates, _, _, fic_t = dmf.run(params, nb_steps)\n",
    "\n",
    "        # Extract the post-burnout part of the FIC timeseries\n",
    "        # fic_t shape: (nodes, time)\n",
    "        burnout_steps = int(math.ceil(burnout * 1000))\n",
    "        fic_t_post_burnout = fic_t[:, burnout_steps:]\n",
    "\n",
    "        # Compute correlation at each time step\n",
    "        # For each time t, correlate fic_t_post_burnout[:, t] with node_strength\n",
    "        for t in range(post_burnout_steps):\n",
    "            fic_vector = fic_t_post_burnout[:, t]\n",
    "            if np.std(fic_vector) == 0 or np.std(node_strength) == 0:\n",
    "                cor = 0  # Handle zero variance\n",
    "            else:\n",
    "                cor = np.corrcoef(fic_vector, node_strength)[0, 1]\n",
    "            all_random_fic_cor[idx_G, t] = cor\n",
    "\n",
    "    # Set the results\n",
    "    result['idx_LR'] = None  # Will be set by the caller based on LR_range\n",
    "    result['all_random_fic_cor'] = all_random_fic_cor\n",
    "\n",
    "    return result\n",
    "\n",
    "def integrate_results(total_tasks, results_folder, fic_cor_timeseries_grid_shape, output_folder):\n",
    "    \"\"\"\n",
    "    Integrates partial results from all tasks and saves the aggregated results.\n",
    "    \"\"\"\n",
    "    print(\"Integrating partial results...\")\n",
    "    \n",
    "    fic_cor_timeseries_grid = np.zeros(fic_cor_timeseries_grid_shape, dtype=np.float32)\n",
    "    \n",
    "    for task_idx in range(total_tasks):\n",
    "        partial_file = os.path.join(results_folder, f\"partial_result_{task_idx}.npy\")\n",
    "        if not os.path.exists(partial_file):\n",
    "            print(f\"Partial result file {partial_file} not found. Skipping.\")\n",
    "            continue\n",
    "        partial_results = np.load(partial_file, allow_pickle=True)\n",
    "        for partial in partial_results:\n",
    "            idx_DECAY = partial['idx_DECAY']\n",
    "            idx_LR = partial['idx_LR']\n",
    "            all_random_fic_cor = partial['all_random_fic_cor']  # shape (len(G_range), post_burnout_steps)\n",
    "\n",
    "            fic_cor_timeseries_grid[idx_DECAY, idx_LR] = all_random_fic_cor\n",
    "\n",
    "    # Save integrated results\n",
    "    arrays_to_save = {\n",
    "        'fic_cor_timeseries_grid': fic_cor_timeseries_grid\n",
    "    }\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for array_name, array_data in arrays_to_save.items():\n",
    "        file_name = os.path.join(output_folder, f\"{array_name}.npy\")\n",
    "        np.save(file_name, array_data)\n",
    "        print(f\"Saved integrated {array_name} to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Distributed FIC Correlation Simulation Script\")\n",
    "    parser.add_argument('--task_idx', type=int, required=True, help='Task index (0 to total_tasks-1)')\n",
    "    parser.add_argument('--seed', type=int, required=True, help='Seed integer - will be used in all jobs')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    task_idx = args.task_idx\n",
    "    total_tasks = 8  # Based on SLURM array=0-7\n",
    "\n",
    "    # Prepare parameters and data\n",
    "    C = loadmat('./data/DTI_fiber_consensus_HCP.mat')['connectivity'][:200, :200]\n",
    "    C = 0.2 * C / np.max(C)\n",
    "    params = dmf.default_params(C=C)\n",
    "\n",
    "    triu_idx = np.triu_indices(C.shape[1], 1)\n",
    "    params['N'] = C.shape[0]\n",
    "    isubfcd = np.triu_indices(C.shape[1], 1)\n",
    "\n",
    "    # Main simulation setup\n",
    "    params[\"return_rate\"] = True\n",
    "    params[\"return_bold\"] = False\n",
    "    params[\"return_fic\"] = True  # Ensure fic_t is returned\n",
    "    params[\"with_plasticity\"] = True\n",
    "    params[\"with_decay\"] = True\n",
    "\n",
    "    G_range = np.arange(0, 8.5, 0.5)\n",
    "    LR_range = np.logspace(0, 3, 100)\n",
    "    DECAY_range = np.logspace(2, 6, 110)\n",
    "\n",
    "    burnout = 7\n",
    "    nb_steps = 50000\n",
    "    NUM_CORES = 24\n",
    "    N_RANDOMIZATIONS = 8  # Removed loop; now handled via seeds\n",
    "\n",
    "    node_strength = C.sum(axis=0)\n",
    "    # Number of time steps after the burnout period\n",
    "    post_burnout_steps = nb_steps - burnout * 1000\n",
    "\n",
    "    \n",
    "    seed = args.seed\n",
    "\n",
    "    # Create a list of argument tuples for the nested loop function\n",
    "    args_list = [(\n",
    "        idx_DECAY,\n",
    "        DECAY,\n",
    "        LR,\n",
    "        seed,\n",
    "        params.copy(),\n",
    "        nb_steps,\n",
    "        burnout,\n",
    "        29,  # overlap\n",
    "        isubfcd,\n",
    "        G_range,\n",
    "        node_strength,\n",
    "        post_burnout_steps\n",
    "    ) for idx_DECAY, DECAY in enumerate(DECAY_range)\n",
    "      for idx_LR, LR in enumerate(LR_range)]\n",
    "\n",
    "    # Define the folder to save partial results\n",
    "    partial_results_folder = f\"./Results/Figure1/FicvsStr3-44-Grid/PartialResults_seed{seed}\"\n",
    "    os.makedirs(partial_results_folder, exist_ok=True)\n",
    "\n",
    "    # Define the folder to save the integrated results\n",
    "    output_folder = f\"./Results/Figure1/FicvsStr3-44-Grid_seed{seed}\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Process assigned (DECAY, LR) pairs using multiprocessing Pool\n",
    "    with Pool(processes=NUM_CORES) as pool:\n",
    "        results = pool.map(grid_step, args_list)\n",
    "\n",
    "    # Prepare data to save\n",
    "    # Each result is a dictionary: {'idx_DECAY': ..., 'idx_LR': ..., 'all_random_fic_cor': ...}\n",
    "    partial_results = []\n",
    "    for res in results:\n",
    "        # Since each task handles one seed, set idx_LR accordingly\n",
    "        idx_DECAY = res['idx_DECAY']\n",
    "        # Find the corresponding idx_LR from the order of args_list\n",
    "        # Assuming DECAY_range is outer loop, LR_range is inner loop\n",
    "        idx_LR = res['idx_LR'] if res['idx_LR'] is not None else None\n",
    "        all_random_fic_cor = res['all_random_fic_cor']\n",
    "        partial_results.append({\n",
    "            'idx_DECAY': idx_DECAY,\n",
    "            'idx_LR': idx_LR,\n",
    "            'all_random_fic_cor': all_random_fic_cor\n",
    "        })\n",
    "\n",
    "    # Save partial results outside the pool\n",
    "    partial_file = os.path.join(partial_results_folder, f\"partial_result_{task_idx}.npy\")\n",
    "    np.save(partial_file, partial_results)\n",
    "    print(f\"Task {task_idx}: Saved partial results to {partial_file}\")\n",
    "\n",
    "    # If this is the designated integrator task, perform integration\n",
    "    # For example, task_idx=0 acts as the integrator\n",
    "    if task_idx == 0:\n",
    "        print(\"Integrator task started. Waiting for all partial results...\")\n",
    "        expected_files = [os.path.join(partial_results_folder, f\"partial_result_{i}.npy\") for i in range(total_tasks)]\n",
    "        \n",
    "        while True:\n",
    "            existing_files = [f for f in expected_files if os.path.exists(f)]\n",
    "            if len(existing_files) >= total_tasks:\n",
    "                print(\"All partial results found. Proceeding to integrate.\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Waiting for partial results... ({len(existing_files)}/{total_tasks} files found)\")\n",
    "                time.sleep(60)  # Wait for 60 seconds before checking again\n",
    "\n",
    "        # Define the shape for fic_cor_timeseries_grid\n",
    "        fic_cor_timeseries_grid_shape = (\n",
    "            len(DECAY_range),\n",
    "            len(LR_range),\n",
    "            len(G_range),\n",
    "            post_burnout_steps\n",
    "        )\n",
    "\n",
    "        # Initialize the aggregated grid\n",
    "        fic_cor_timeseries_grid = np.zeros(fic_cor_timeseries_grid_shape, dtype=np.float32)\n",
    "\n",
    "        for task_idx_integrate in range(total_tasks):\n",
    "            partial_file_integrate = os.path.join(partial_results_folder, f\"partial_result_{task_idx_integrate}.npy\")\n",
    "            if not os.path.exists(partial_file_integrate):\n",
    "                print(f\"Partial result file {partial_file_integrate} not found. Skipping.\")\n",
    "                continue\n",
    "            partial_results_integrate = np.load(partial_file_integrate, allow_pickle=True)\n",
    "            for partial in partial_results_integrate:\n",
    "                idx_DECAY = partial['idx_DECAY']\n",
    "                idx_LR = partial['idx_LR']\n",
    "                all_random_fic_cor = partial['all_random_fic_cor']  # shape (len(G_range), post_burnout_steps)\n",
    "\n",
    "                # Aggregate the correlations by averaging across seeds\n",
    "                # Since each task corresponds to one seed, we'll average over seeds\n",
    "                fic_cor_timeseries_grid[idx_DECAY, idx_LR] += all_random_fic_cor\n",
    "\n",
    "        # Average the aggregated correlations by the number of randomizations\n",
    "        fic_cor_timeseries_grid /= N_RANDOMIZATIONS\n",
    "\n",
    "        # Save the integrated results\n",
    "        arrays_to_save = {\n",
    "            'fic_cor_timeseries_grid': fic_cor_timeseries_grid\n",
    "        }\n",
    "\n",
    "        for array_name, array_data in arrays_to_save.items():\n",
    "            file_name = os.path.join(output_folder, f\"{array_name}.npy\")\n",
    "            np.save(file_name, array_data)\n",
    "            print(f\"Saved integrated {array_name} to {file_name}\")\n",
    "\n",
    "        print(\"Integration completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
