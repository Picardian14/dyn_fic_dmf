{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, lfilter\n",
    "import fastdyn_fic_dmf as dmf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Fetch default parameters\n",
    "import tracemalloc\n",
    "from scipy.io import loadmat\n",
    "from scipy.stats import zscore, pearsonr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mat73\n",
    "# Helper functions\n",
    "def compute_fcd(data, wsize, overlap, isubdiag):\n",
    "    T, N = data.shape\n",
    "    win_start = np.arange(0, T - wsize - 1, wsize - overlap)\n",
    "    nwins = len(win_start)\n",
    "    fcd = np.zeros((len(isubdiag[0]), nwins))\n",
    "    #print(fcd.shape)\n",
    "    #print(data.shape)\n",
    "    #print((data[win_start[2]:win_start[2] + wsize + 1, :]).shape)\n",
    "    for i in range(nwins):\n",
    "        tmp = data[win_start[i]:win_start[i] + wsize + 1, :]\n",
    "        cormat = np.corrcoef(tmp.T)\n",
    "        fcd[:, i] = cormat[isubdiag[0],isubdiag[1]]\n",
    "    return fcd\n",
    "\n",
    "\n",
    "C = loadmat('./data/DTI_fiber_consensus_HCP.mat')['connectivity'][:200, :200]\n",
    "C = 0.2*C/np.max(C)\n",
    "N = C.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sim_run(G_VAL, LR, SEED, NB_STEPS=50000):\n",
    "    \"\"\"\n",
    "    INPUTS:\n",
    "    G_VAL: float, global coupling\n",
    "    LR: array, learning rate (Homogeneous or heterogenos. Decay will be calcualted for each region with this)\n",
    "    SEED: int, random seed\n",
    "    OUTPUTS:\n",
    "    rates_dyn: np.array, dynamic of rates\n",
    "    rates_inh_dyn: np.array, dynamic of inhibitory rates\n",
    "    bold_dyn: np.array, dynamic of BOLD signal\n",
    "    fic_t_dyn: np.array, dynamic of FIC\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    params = dmf.default_params(C=C)\n",
    "    fit_res = np.load(\"./data/fit_res_3-44.npy\")\n",
    "    b = fit_res[0] # First element is the slope\n",
    "    a = fit_res[1]\n",
    "    params['G'] = G_VAL\n",
    "    params['seed'] = SEED\n",
    "    params['obj_rate'] = 3.44\n",
    "    DECAY = np.exp(a+np.log(LR)*b)    \n",
    "    params['lr_vector'] = LR\n",
    "    params['taoj_vector'] =  DECAY\n",
    "    params['J'] = 0.75*params['G']*params['C'].sum(axis=0).squeeze() + 1\n",
    "    params[\"with_decay\"] = True\n",
    "    params[\"with_plasticity\"] = True\n",
    "    params['return_bold'] = False\n",
    "    params[\"return_fic\"] = False\n",
    "    params[\"return_rate\"] = True\n",
    "    rates_dyn, rates_inh_dyn, _, fic_t_dyn = dmf.run(params, NB_STEPS)\n",
    "    return rates_dyn, rates_inh_dyn, fic_t_dyn\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_along_axis(axis=0):\n",
    "    def decorator(func):\n",
    "        def wrapper(data, *args, **kwargs):\n",
    "            # if the data is 1D, just call the function directly\n",
    "            if data.ndim == 1:\n",
    "                return func(data, *args, **kwargs)\n",
    "            # otherwise, apply the function along the specified axis\n",
    "            return np.apply_along_axis(func, axis, data, *args, **kwargs)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "@vectorize_along_axis(axis=0)\n",
    "def get_autcorr(rates):\n",
    "    \"\"\"Get the autocorrelation function from a 1D rates vector.\"\"\"\n",
    "    signal = rates - np.mean(rates)\n",
    "    # Calculate autocorrelation function (full convolution)\n",
    "    autocorr = np.correlate(signal, signal, mode='full')\n",
    "    # Normalize: divide by the variance and length of the signal\n",
    "    autocorr = autocorr / (np.var(signal) * len(signal))\n",
    "    # Only keep the second half (non-negative lags)\n",
    "    autocorr = autocorr[len(signal)-1:]\n",
    "    return autocorr\n",
    "# create a function that computes and plots the autocorrelation of the average rates\n",
    "def plot_autocorr(rates, title):\n",
    "    autocorr = get_autcorr(np.mean(rates, axis=0))\n",
    "    lags = np.arange(0, len(autocorr))\n",
    "    plt.plot(lags, autocorr)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Lag')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved homogeneous simulation results for LR = 500 to ./Results/homogeneous/g_3.5_lr_500.npy\n",
      "Saved homogeneous simulation results for LR = 15000 to ./Results/homogeneous/g_3.5_lr_15000.npy\n",
      "Saved homogeneous simulation results for LR = 20000 to ./Results/homogeneous/g_3.5_lr_20000.npy\n",
      "Saved homogeneous simulation results for LR = 25000 to ./Results/homogeneous/g_3.5_lr_25000.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "G_VAL = 3.5\n",
    "NB_STEPS = 15000\n",
    "BURNOUT = 5000\n",
    "SEED_BASE = 1  # Base seed value\n",
    "\n",
    "# Define the list of LR values you want to test\n",
    "lr_values = [500, 15000, 20000, 25000]\n",
    "\n",
    "# Assuming N is defined globally (number of regions)\n",
    "N = 200  # adjust accordingly if different\n",
    "\n",
    "def run_simulation(idx, lr):\n",
    "    SEED = SEED_BASE + idx\n",
    "    # Create a homogeneous LR vector for all regions\n",
    "    LR_VEC = np.ones(N) * lr\n",
    "    # Run simulation (assuming sim_run returns rates, inhibitory rates and fic_t in that order)\n",
    "    rates, _, _ = sim_run(G_VAL, LR_VEC, SEED, NB_STEPS)\n",
    "    # Discard burnout period\n",
    "    return rates[:, BURNOUT:]\n",
    "\n",
    "for lr in lr_values:\n",
    "    # Execute 100 simulation runs in parallel.\n",
    "    simulations = Parallel(n_jobs=16)(delayed(run_simulation)(idx, lr) for idx in range(100))\n",
    "    rates_all = np.array(simulations)\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(\"./Results/homogeneous\", exist_ok=True)\n",
    "    filename = f\"./Results/homogeneous/g_{G_VAL}_lr_{lr}.npy\"\n",
    "    np.save(filename, rates_all)\n",
    "    print(f\"Saved homogeneous simulation results for LR = {lr} to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [1/75], Loss: 6.5635\n",
      "Epoch [1/2], Step [6/75], Loss: 6.5785\n",
      "Epoch [1/2], Step [11/75], Loss: 6.5530\n",
      "Epoch [1/2], Step [16/75], Loss: 6.5599\n",
      "Epoch [1/2], Step [21/75], Loss: 6.6669\n",
      "Epoch [1/2], Step [26/75], Loss: 6.5850\n",
      "Epoch [1/2], Step [31/75], Loss: 6.5389\n",
      "Epoch [1/2], Step [36/75], Loss: 6.4730\n",
      "Epoch [1/2], Step [41/75], Loss: 6.5729\n",
      "Epoch [1/2], Step [46/75], Loss: 6.4165\n",
      "Epoch [1/2], Step [51/75], Loss: 6.5402\n",
      "Epoch [1/2], Step [56/75], Loss: 6.3737\n",
      "Epoch [1/2], Step [61/75], Loss: 6.4613\n",
      "Epoch [1/2], Step [66/75], Loss: 6.4587\n",
      "Epoch [1/2], Step [71/75], Loss: 6.4406\n",
      "=== End of Epoch 1, Avg Loss: 6.5461 ===\n",
      "Epoch [2/2], Step [1/75], Loss: 6.5271\n",
      "Epoch [2/2], Step [6/75], Loss: 6.4593\n",
      "Epoch [2/2], Step [11/75], Loss: 6.6084\n",
      "Epoch [2/2], Step [16/75], Loss: 6.5327\n",
      "Epoch [2/2], Step [21/75], Loss: 6.4393\n",
      "Epoch [2/2], Step [26/75], Loss: 6.5737\n",
      "Epoch [2/2], Step [31/75], Loss: 6.5784\n",
      "Epoch [2/2], Step [36/75], Loss: 6.4851\n",
      "Epoch [2/2], Step [41/75], Loss: 6.4542\n",
      "Epoch [2/2], Step [46/75], Loss: 6.5722\n",
      "Epoch [2/2], Step [51/75], Loss: 6.4635\n",
      "Epoch [2/2], Step [56/75], Loss: 6.4464\n",
      "Epoch [2/2], Step [61/75], Loss: 6.5143\n",
      "Epoch [2/2], Step [66/75], Loss: 6.3469\n",
      "Epoch [2/2], Step [71/75], Loss: 6.3349\n",
      "=== End of Epoch 2, Avg Loss: 6.4767 ===\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.signal as signal\n",
    "\n",
    "############################\n",
    "# 1) PhaseDiffDataset\n",
    "############################\n",
    "class PhaseDiffDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Converts raw 'rates' of shape (N, 200, 50000) into sequences of 20 phase-difference\n",
    "    matrices, each 200x200. Final shape: (N, 20, 1, 200, 200).\n",
    "    \"\"\"\n",
    "    def __init__(self, rates, num_timepoints=20):\n",
    "        \"\"\"\n",
    "        rates: NumPy array of shape (N=150, 200, 50000).\n",
    "        num_timepoints: how many equally spaced time points to sample for each simulation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_timepoints = num_timepoints\n",
    "        self.phase_diff = self._create_phase_diff_matrices(rates, num_timepoints)\n",
    "\n",
    "    def _create_phase_diff_matrices(self, rates, num_matrices):\n",
    "        \"\"\"\n",
    "        1) Apply Hilbert transform -> instantaneous phase\n",
    "        2) Sample 'num_matrices' time points from each simulation\n",
    "        3) Build 200x200 phase-difference matrix at each time point\n",
    "        Returns a NumPy array of shape (N, num_matrices, 1, 200, 200).\n",
    "        \"\"\"\n",
    "        N, num_channels, timesteps = rates.shape  # e.g. (150, 200, 50000)\n",
    "\n",
    "        # Hilbert transform along time axis -> instantaneous phase\n",
    "        analytic_signal = signal.hilbert(rates, axis=2)    # shape (N, 200, 50000)\n",
    "        inst_phase = np.angle(analytic_signal)             # same shape\n",
    "\n",
    "        # Time indices for sampling\n",
    "        time_indices = np.linspace(0, timesteps - 1, num_matrices, dtype=int)\n",
    "\n",
    "        all_phase_diff = []\n",
    "        for i in range(N):\n",
    "            # Extract the phase for the ith simulation -> shape (200, 50000)\n",
    "            phase_i = inst_phase[i]\n",
    "\n",
    "            # Collect phase-diff matrices for the 20 time points\n",
    "            diff_list = []\n",
    "            for t in time_indices:\n",
    "                phase_t = phase_i[:, t]  # shape (200,)\n",
    "                diff_matrix = phase_t[:, None] - phase_t[None, :]  # (200, 200)\n",
    "                diff_list.append(diff_matrix[None, ...])          # (1, 200, 200)\n",
    "\n",
    "            # Shape -> (20, 200, 200) for this simulation\n",
    "            diff_array = np.concatenate(diff_list, axis=0)\n",
    "            # Keep for each simulation\n",
    "            all_phase_diff.append(diff_array[None, ...])   # (1, 20, 200, 200)\n",
    "\n",
    "        # Combine => (N, 20, 200, 200)\n",
    "        all_phase_diff = np.concatenate(all_phase_diff, axis=0)\n",
    "        # Add channel dimension => (N, 20, 1, 200, 200)\n",
    "        all_phase_diff = all_phase_diff[:, :, None, :, :]\n",
    "        return all_phase_diff\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.phase_diff.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a float tensor of shape (20, 1, 200, 200)\n",
    "        return torch.from_numpy(self.phase_diff[idx]).float()\n",
    "\n",
    "\n",
    "############################\n",
    "# 2) ConvLSTM building blocks\n",
    "############################\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A single ConvLSTM cell for 2D frames.\n",
    "    Input shape: (B, in_channels, H, W)\n",
    "    Output: h_next, c_next\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        \n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=input_channels + hidden_channels,\n",
    "            out_channels=4 * hidden_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding\n",
    "        )\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        # x, h => (B, channels, H, W)\n",
    "        combined = torch.cat([x, h], dim=1)  # concat along channel axis\n",
    "        gates = self.conv(combined)         # => (B, 4*hidden_channels, H, W)\n",
    "        \n",
    "        # Split into i, f, g, o\n",
    "        i, f, g, o = torch.split(gates, self.hidden_channels, dim=1)\n",
    "        \n",
    "        i = torch.sigmoid(i)\n",
    "        f = torch.sigmoid(f)\n",
    "        g = torch.tanh(g)\n",
    "        o = torch.sigmoid(o)\n",
    "        \n",
    "        c_next = f * c + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        \n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, spatial_size):\n",
    "        height, width = spatial_size\n",
    "        device = next(self.parameters()).device\n",
    "        h = torch.zeros(batch_size, self.hidden_channels, height, width, device=device)\n",
    "        c = torch.zeros(batch_size, self.hidden_channels, height, width, device=device)\n",
    "        return h, c\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Single-layer ConvLSTM that processes a sequence of 2D frames:\n",
    "      Input shape: (B, T, in_channels, H, W)\n",
    "      Output: \n",
    "        if return_sequence=True -> (B, T, hidden_channels, H, W)\n",
    "        else -> final (h, c)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size=3, padding=1, return_sequence=False):\n",
    "        super().__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.cell = ConvLSTMCell(input_channels, hidden_channels, kernel_size, padding)\n",
    "        self.return_sequence = return_sequence\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, timesteps, _, height, width = x.size()\n",
    "        h, c = self.cell.init_hidden(batch_size, (height, width))\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(timesteps):\n",
    "            x_t = x[:, t]          # shape: (B, in_channels, H, W)\n",
    "            h, c = self.cell(x_t, h, c)\n",
    "            if self.return_sequence:\n",
    "                outputs.append(h.unsqueeze(1))  # store each time step's h\n",
    "        \n",
    "        if self.return_sequence:\n",
    "            return torch.cat(outputs, dim=1), (h, c)\n",
    "        else:\n",
    "            return h, c\n",
    "\n",
    "############################\n",
    "# 3) ConvLSTMAutoEncoder\n",
    "############################\n",
    "class ConvLSTMAutoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Shallow autoencoder with:\n",
    "      - encoder_lstm (no sequence output)\n",
    "      - decoder_lstm (returns the entire sequence)\n",
    "      - final conv to map hidden_channels -> input_channels\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels=1, hidden_channels=8, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.encoder_lstm = ConvLSTM(\n",
    "            input_channels=input_channels,\n",
    "            hidden_channels=hidden_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            return_sequence=False\n",
    "        )\n",
    "        self.decoder_lstm = ConvLSTM(\n",
    "            input_channels=hidden_channels,\n",
    "            hidden_channels=hidden_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            return_sequence=True\n",
    "        )\n",
    "        self.conv_out = nn.Conv2d(hidden_channels, input_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T=20, in_channels=1, H=200, W=200)\n",
    "        Returns reconstruction => same shape (B, T, 1, H, W).\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = x.shape\n",
    "        \n",
    "        # --- Encoder ---\n",
    "        h_enc, c_enc = self.encoder_lstm(x)   # final hidden/cell from encoder\n",
    "        \n",
    "        # --- Decoder ---\n",
    "        # We'll feed zeros at each time step (could use teacher forcing if desired)\n",
    "        decoder_input = torch.zeros((B, T, self.decoder_lstm.hidden_channels, H, W), \n",
    "                                    device=x.device, dtype=x.dtype)\n",
    "        h_dec, c_dec = (h_enc, c_enc)\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(T):\n",
    "            x_t = decoder_input[:, t]\n",
    "            h_dec, c_dec = self.decoder_lstm.cell(x_t, h_dec, c_dec)\n",
    "            outputs.append(h_dec.unsqueeze(1))\n",
    "        \n",
    "        # shape => (B, T, hidden_channels, H, W)\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        \n",
    "        # map each frame to 1 channel\n",
    "        recon_frames = []\n",
    "        for t in range(T):\n",
    "            out_t = self.conv_out(outputs[:, t])\n",
    "            recon_frames.append(out_t.unsqueeze(1))\n",
    "        \n",
    "        # shape => (B, T, 1, H, W)\n",
    "        recon_sequence = torch.cat(recon_frames, dim=1)\n",
    "        return recon_sequence\n",
    "\n",
    "############################\n",
    "# 4) Main script\n",
    "############################\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparams\n",
    "    batch_size = 2\n",
    "    lr = 1e-3\n",
    "    num_epochs = 2\n",
    "    \n",
    "    # Suppose we have 150 simulations: shape (150, 200, 50000)\n",
    "    rates = np.random.randn(150, 200, 10000)\n",
    "\n",
    "    # Build dataset -> shape (150, 20, 1, 200, 200)\n",
    "    dataset = PhaseDiffDataset(rates, num_timepoints=20)\n",
    "    \n",
    "    # Wrap in DataLoader\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Instantiate model\n",
    "    model = ConvLSTMAutoEncoder(input_channels=1, hidden_channels=8)  # smaller hidden_channels to save memory\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for i, batch in enumerate(loader):\n",
    "            # batch: (B, 20, 1, 200, 200)\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            recon = model(batch)    # shape (B, 20, 1, 200, 200)\n",
    "            loss = criterion(recon, batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            if i % 5 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(loader)}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = total_loss / len(loader)\n",
    "        print(f\"=== End of Epoch {epoch+1}, Avg Loss: {avg_loss:.4f} ===\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fic_h",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
