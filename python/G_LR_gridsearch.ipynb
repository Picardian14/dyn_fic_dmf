{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Custom modules (as in your snippet)\n",
    "import fastdyn_fic_dmf as dmf\n",
    "from helper_functions import filter_bold\n",
    "\n",
    "################################################################################\n",
    "# Helper functions\n",
    "################################################################################\n",
    "\n",
    "def compute_fc(data):\n",
    "    \"\"\"\n",
    "    Compute FC by correlating BOLD (or rate) time series.\n",
    "    data shape: (time, nodes).\n",
    "    Returns an (N x N) correlation matrix.\n",
    "    \"\"\"\n",
    "    return np.corrcoef(data.T)  # shape [N, N]\n",
    "\n",
    "def compute_fcd(data, wsize, overlap, isubdiag):\n",
    "    \"\"\"\n",
    "    Compute FCD by sliding a window of length 'wsize' with 'overlap' into 'data'.\n",
    "    \n",
    "    data shape: (time, nodes).\n",
    "    wsize, overlap: integer time points.\n",
    "    isubdiag: np.triu_indices(N, 1) or similar.\n",
    "\n",
    "    Returns a 2D array:\n",
    "        shape [num_windows, number_of_corr_pairs]\n",
    "    where number_of_corr_pairs = len(isubdiag[0]).\n",
    "    \"\"\"\n",
    "    T, N = data.shape\n",
    "    step = wsize - overlap\n",
    "    win_starts = np.arange(0, T - wsize + 1, step)\n",
    "    \n",
    "    fcd_mat = []\n",
    "    for start in win_starts:\n",
    "        window_data = data[start:start + wsize, :]\n",
    "        cormat = np.corrcoef(window_data.T)\n",
    "        fcd_mat.append(cormat[isubdiag])\n",
    "    fcd_mat = np.corrcoef(np.array(fcd_mat))  # shape [num_windows, n_subdiag]\n",
    "    return fcd_mat  # shape [num_windows, n_subdiag]\n",
    "\n",
    "\n",
    "\n",
    "def simulate_one_seed(args):\n",
    "    \"\"\"\n",
    "    Run the DMF model for a single seed, given LR and G (plus a,b for DECAY),\n",
    "    and return (FC, FCD).\n",
    "    \"\"\"\n",
    "    (params_base, nb_steps, burnout, wsize, overlap, seed_id, a, b, isubdiag) = args\n",
    "\n",
    "    # Copy base params so we don't mutate them\n",
    "    params = params_base.copy()\n",
    "    params['seed'] = seed_id\n",
    "    \n",
    "    # Compute DECAY from a, b, lrj\n",
    "    DECAY = np.exp(a + np.log(params['lrj']) * b)\n",
    "    params['taoj'] = DECAY\n",
    "    \n",
    "    # Because we want to see plastic changes\n",
    "    params[\"with_plasticity\"] = True\n",
    "    params[\"with_decay\"]      = True\n",
    "    \n",
    "    # (Re)compute 'J' after setting alpha, G, etc.\n",
    "    # If alpha is always 0.75 or you prefer a different logic, do it here:\n",
    "    params['alpha'] = 0.75\n",
    "    params['J'] = params['alpha'] * params['G'] * params['C'].sum(axis=0).squeeze() + 1\n",
    "\n",
    "    # Run DMF\n",
    "    # Returns: (rates, bold, fic, etc.) -- adjust if your function differs\n",
    "    try:\n",
    "        _, _, bold, _ = dmf.run(params, nb_steps)\n",
    "        \n",
    "        # Take out the nan values\n",
    "        bold[np.isnan(bold)] = 0\n",
    "        # Burn out the initial transients\n",
    "        bold_post = bold[:, burnout:]  # shape [N, T-burnout]\n",
    "        # If we need to filter the BOLD, do it here\n",
    "        # Convert to shape [time, nodes] for correlation\n",
    "        bold_post = bold_post.T  # shape [T-burnout, N]\n",
    "        bold_filt = filter_bold(bold_post, params['flp'], params['fhp'], params['TR'])\n",
    "\n",
    "        # Compute FC\n",
    "        fc_seed = compute_fc(bold_filt)  # shape [N, N]\n",
    "\n",
    "        # Compute FCD\n",
    "        fcd_seed = compute_fcd(bold_filt, wsize, overlap, isubdiag)  \n",
    "    except:\n",
    "        print(f\"Error in seed {seed_id}, returning zeros.\")\n",
    "        print(f\"G={params['G']}, LR={params['lrj']}\")\n",
    "        fc_seed = np.zeros((params['N'], params['N']), dtype=np.float32)\n",
    "        fcd_seed = np.zeros((1, 1), dtype=np.float32)\n",
    "    # shape [num_windows, n_subdiag]\n",
    "\n",
    "    return fc_seed, fcd_seed\n",
    "\n",
    "def grid_step(args):\n",
    "    \"\"\"\n",
    "    Process a single (LR, G) pair by running multiple seeds.\n",
    "    Return:\n",
    "      {\n",
    "        'idx_lr': idx_lr,\n",
    "        'idx_g':  idx_g,\n",
    "        'FC_avg':  (N x N),\n",
    "        'FCD_stacked': (n_seeds x num_windows x n_subdiag)\n",
    "      }\n",
    "    \"\"\"\n",
    "    (idx_lr, LR_val, idx_g, G_val,\n",
    "     params, nb_steps, burnout, wsize, overlap, n_seeds,\n",
    "     a, b, isubdiag) = args\n",
    "\n",
    "    # Set the LR & G in the param dictionary\n",
    "    params['lrj'] = LR_val\n",
    "    params['G']   = G_val\n",
    "    \n",
    "    N = params['N']\n",
    "\n",
    "    # We'll sum FCs to get an average later\n",
    "    fc_sum = np.zeros((N, N), dtype=np.float32)\n",
    "    fcd_list = []\n",
    "\n",
    "    # Prepare a local list of seeds\n",
    "    # Example: we might do seeds = range(n_seeds), or something more elaborate\n",
    "    # For demonstration, we just do seeds 0..(n_seeds-1)\n",
    "    seed_list = range(n_seeds)\n",
    "    \n",
    "    # Build arguments for simulate_one_seed\n",
    "    simulate_args = []\n",
    "    for seed_id in seed_list:\n",
    "        # Some unique seed scheme:\n",
    "        # e.g. seed_in = seed_id + 1000 * idx_lr + 10000 * idx_g\n",
    "        seed_in = seed_id + idx_lr + 2 * idx_g\n",
    "        simulate_args.append((\n",
    "            params,           # base param\n",
    "            nb_steps,\n",
    "            burnout,\n",
    "            wsize,\n",
    "            overlap,\n",
    "            seed_in,\n",
    "            a,\n",
    "            b,\n",
    "            isubdiag\n",
    "        ))\n",
    "    \n",
    "    # Run seeds in parallel\n",
    "    NWORKERS = 16\n",
    "    fcs = []\n",
    "    with Pool(NWORKERS) as local_pool:\n",
    "        results = local_pool.map(simulate_one_seed, simulate_args)\n",
    "    # Kill Pool\n",
    "    local_pool.close()\n",
    "\n",
    "    # Aggregate\n",
    "    for fc_seed, fcd_seed in results:\n",
    "        fc_sum += fc_seed\n",
    "        fcd_list.append(fcd_seed)\n",
    "        fcs.append(fc_seed)\n",
    "\n",
    "    # Average FC\n",
    "    \n",
    "    fc_avg = fc_sum / n_seeds\n",
    "    \n",
    "    # Stack FCD => shape [n_seeds, num_windows, n_subdiag]\n",
    "    fcd_stacked = np.stack(fcd_list, axis=0)\n",
    "\n",
    "    return {\n",
    "        'idx_lr': idx_lr,\n",
    "        'idx_g':  idx_g,\n",
    "        'FC_avg': fc_avg,\n",
    "        'FCD_stacked': fcd_stacked,\n",
    "        'fcs': fcs\n",
    "    }\n",
    "\n",
    "################################################################################\n",
    "# Integration of partial results\n",
    "################################################################################\n",
    "\n",
    "def integrate_results(total_tasks, results_folder,\n",
    "                      nLR, nG, n_seeds, output_folder):\n",
    "    \"\"\"\n",
    "    Loads partial results from partial_result_0..(total_tasks-1).npy\n",
    "    and constructs final arrays:\n",
    "      FC_grid:  (nLR, nG, N, N)\n",
    "      FCD_grid: (nLR, nG, n_seeds, num_windows, n_subdiag)\n",
    "\n",
    "    Then saves them to output_folder.\n",
    "    \"\"\"\n",
    "    print(\"[integrate_results] Integrating partial results...\")\n",
    "\n",
    "    FC_grid = None\n",
    "    FCD_grid = None\n",
    "    loaded_something = False\n",
    "\n",
    "    for task_idx in range(total_tasks):\n",
    "        partial_file = os.path.join(results_folder, f\"partial_result_{task_idx}.npy\")\n",
    "        if not os.path.exists(partial_file):\n",
    "            print(f\"  [Warning] partial file not found: {partial_file}\")\n",
    "            continue\n",
    "        \n",
    "        # Each partial is a list of dicts: { 'idx_lr', 'idx_g', 'FC_avg', 'FCD_stacked' }\n",
    "        partial_data = np.load(partial_file, allow_pickle=True)\n",
    "        \n",
    "        for item in partial_data:\n",
    "            idx_lr = item['idx_lr']\n",
    "            idx_g  = item['idx_g']\n",
    "            fc_avg = item['FC_avg']       # shape [N, N]\n",
    "            fcd_st = item['FCD_stacked']  # shape [n_seeds, num_windows, n_subdiag]\n",
    "\n",
    "            if not loaded_something:\n",
    "                N = fc_avg.shape[0]\n",
    "                num_windows = fcd_st.shape[1]\n",
    "                n_subdiag  = fcd_st.shape[2]\n",
    "\n",
    "                FC_grid  = np.zeros((nLR, nG, N, N), dtype=np.float32)\n",
    "                FCD_grid = np.zeros((nLR, nG, n_seeds, num_windows, num_windows), dtype=np.float32)\n",
    "                loaded_something = True\n",
    "\n",
    "            FC_grid[idx_lr, idx_g]  = fc_avg\n",
    "            FCD_grid[idx_lr, idx_g] = fcd_st\n",
    "\n",
    "    # Save final results\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    if FC_grid is not None:\n",
    "        np.save(os.path.join(output_folder, \"FC_grid.npy\"), FC_grid)\n",
    "        print(\"[integrate_results] Saved FC_grid\")\n",
    "    if FCD_grid is not None:\n",
    "        np.save(os.path.join(output_folder, \"FCD_grid.npy\"), FCD_grid)\n",
    "        print(\"[integrate_results] Saved FCD_grid\")\n",
    "\n",
    "    print(\"[integrate_results] Done.\")\n",
    "\n",
    "################################################################################\n",
    "# Main script\n",
    "################################################################################\n",
    "\n",
    "\n",
    "# Slurm-like config\n",
    "total_tasks = 24\n",
    "task_idx = 1#args.task_idx\n",
    "\n",
    "    # Load structural connectivity\n",
    "C = loadmat('./data/DTI_fiber_consensus_HCP.mat')['connectivity'][:200,:200]\n",
    "C = 0.2 * C / np.max(C)\n",
    "\n",
    "# Base DMF params\n",
    "params = dmf.default_params(C=C)\n",
    "params['N'] = C.shape[0]\n",
    "N = params['N']\n",
    "\n",
    "# Filtering params (adapt to your usage)\n",
    "params[\"flp\"] = 0.01\n",
    "params[\"fhp\"] = 0.1\n",
    "params[\"TR\"]  = 2\n",
    "\n",
    "# For windowed FCD\n",
    "wsize   = 30\n",
    "overlap = 29\n",
    "\n",
    "# Burnout in time points (or directly # of samples)\n",
    "burnout = 7  # if your run uses 1 step = 1 ms, might be burnout * 1000\n",
    "                # but adapt to how your model is defined\n",
    "\n",
    "# Total simulation time in TRs:\n",
    "T = 250\n",
    "# We assume dtt from your snippet:\n",
    "params['dtt'] = 0.001\n",
    "nb_steps = int(T * params[\"TR\"] / params[\"dtt\"])\n",
    "\n",
    "# Indices for the upper triangular part (for FC or FCD)\n",
    "isubdiag = np.triu_indices(N, 1)\n",
    "\n",
    "# Load slope and intercept for DECAY\n",
    "fit_res  = np.load(\"./data/fit_res_3-44.npy\", allow_pickle=True)\n",
    "b = fit_res[0]  # slope\n",
    "a = fit_res[1]  # intercept\n",
    "\n",
    "# Grid definitions\n",
    "nLR = 110\n",
    "LR_range = np.logspace(0, 3, nLR)   # from 1 to 1000\n",
    "nG  = 100\n",
    "G_range = np.linspace(0.1, 16, nG)  # from 0.1 to 16\n",
    "\n",
    "# Seeds per (LR, G)\n",
    "n_seeds = 16\n",
    "\n",
    "# Build the entire list of (LR, G) pairs\n",
    "# Build the entire list of (LR, G) pairs\n",
    "all_args = []\n",
    "for idx_lr, LR_val in enumerate(LR_range):\n",
    "    for idx_g, G_val in enumerate(G_range):\n",
    "        # We pass everything needed, including a,b\n",
    "        all_args.append((\n",
    "            idx_lr, LR_val,\n",
    "            idx_g,  G_val,\n",
    "            params.copy(),\n",
    "            nb_steps,\n",
    "            burnout,\n",
    "            wsize,\n",
    "            overlap,\n",
    "            n_seeds,\n",
    "            a, b,\n",
    "            isubdiag\n",
    "        ))\n",
    "\n",
    "# Distribute (LR, G) pairs among tasks\n",
    "total_pairs = len(all_args)\n",
    "chunk_size = math.ceil(total_pairs / total_tasks)\n",
    "start_idx = task_idx * chunk_size\n",
    "end_idx   = min(start_idx + chunk_size, total_pairs)\n",
    "task_args = all_args[start_idx:end_idx]\n",
    "\n",
    "# Folder for partial results\n",
    "partial_results_folder = \"./Results/Partial_Grid_LR_G\"\n",
    "os.makedirs(partial_results_folder, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "459"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debuging boldDebuging boldDebuging boldDebuging boldDebuging boldDebuging boldDebuging boldDebuging boldDebuging boldDebuging boldDebuging boldDebuging boldDebuging boldDebuging boldDebuging boldDebuging bold"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg_tuple \u001b[38;5;129;01min\u001b[39;00m task_args:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# grid_step() itself parallelizes across seeds\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_tuple\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Save partial results\u001b[39;00m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mgrid_step\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    155\u001b[0m fcs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(NWORKERS) \u001b[38;5;28;01mas\u001b[39;00m local_pool:\n\u001b[0;32m--> 157\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mlocal_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimulate_one_seed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimulate_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Kill Pool\u001b[39;00m\n\u001b[1;32m    159\u001b[0m local_pool\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.conda/envs/fic/lib/python3.10/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/fic/lib/python3.10/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/fic/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/fic/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.conda/envs/fic/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Each task processes its chunk of (LR, G) pairs in SERIAL,\n",
    "# but seeds are parallelized in grid_step().\n",
    "results = []\n",
    "for arg_tuple in task_args:\n",
    "    # grid_step() itself parallelizes across seeds\n",
    "    result = grid_step(arg_tuple)\n",
    "    results.append(result)\n",
    "\n",
    "# Save partial results\n",
    "partial_file = os.path.join(partial_results_folder, f\"partial_result_{task_idx}.npy\")\n",
    "np.save(partial_file, results)\n",
    "print(f\"[main] Task {task_idx} saved partial results: {partial_file}\")\n",
    "\n",
    "# Integrator on task_idx=0\n",
    "if task_idx == 0:\n",
    "    print(\"[main] Integrator waiting for other tasks...\")\n",
    "    expected_files = [\n",
    "        os.path.join(partial_results_folder, f\"partial_result_{i}.npy\")\n",
    "        for i in range(total_tasks)\n",
    "    ]\n",
    "    while True:\n",
    "        existing = [f for f in expected_files if os.path.exists(f)]\n",
    "        if len(existing) == total_tasks:\n",
    "            print(\"[main] All partial results found. Proceeding to integration.\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"[main] {len(existing)}/{total_tasks} partial files found, waiting...\")\n",
    "            time.sleep(30)\n",
    "\n",
    "    # Integrate them into final arrays\n",
    "    output_folder = \"./Results/FC_FCD_Grid\"\n",
    "    integrate_results(total_tasks,\n",
    "                        partial_results_folder,\n",
    "                        nLR, nG, n_seeds,\n",
    "                        output_folder)\n",
    "    print(\"[main] Integration completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1375"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(task_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
